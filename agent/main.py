import sys
import os

import io
from langchain_community.chat_models import  ChatOpenAI
from langchain.chat_models import init_chat_model
#from agent.rag_setup import get_vector_store, create_rag_tool
import streamlit as st
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
from agent.graph_workflow import create_graph
from rag_setup import get_vector_store, create_rag_tool

                
def run_agent(review_text: str):
    """
    è¿è¡Œæ™ºèƒ½è¯„è®ºå›å¤ Agent
    """
    # åˆ›å»ºå¹¶ç¼–è¯‘å›¾
    app = create_graph()
    
    # å‡†å¤‡è¾“å…¥
    inputs = {"original_review": review_text}
    
    # è¿è¡Œ Agent
    print(f"\n--- å¼€å§‹å¤„ç†æ–°è¯„è®º: '{review_text}' ---")
    result = app.invoke(inputs)
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n--- Agent æœ€ç»ˆå›å¤ ---")
    print(result["finally_reply"])
    return result


if __name__ == "__main__":
    # # --- åœ¨è¿™é‡Œæµ‹è¯•ä½ çš„ Agent ---
    
    # # æµ‹è¯•ç”¨ä¾‹1: è´Ÿé¢è¯„ä»·ï¼Œéœ€è¦è°ƒç”¨å·¥å…·
    # negative_review_with_tool = "è¿™ä¸ªäº§å“å¤ªå·®äº†ï¼Œæˆ‘ä¸çŸ¥é“æ€ä¹ˆç”¨ï¼è¯·å¸®æˆ‘çœ‹çœ‹è¯´æ˜ä¹¦ã€‚"
    
    # run_agent(negative_review_with_tool)
    
    # print("\n" + "="*50 + "\n")

    # # æµ‹è¯•ç”¨ä¾‹2: è´Ÿé¢è¯„ä»·ï¼Œæ— éœ€è°ƒç”¨å·¥å…·
    # negative_review_no_tool = "ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†åŠä¸ªæœˆæ‰åˆ°ï¼å·®è¯„ï¼"
    # run_agent(negative_review_no_tool)

    # print("\n" + "="*50 + "\n")

    # # æµ‹è¯•ç”¨ä¾‹3: æ­£é¢è¯„ä»·
    # positive_review = "è´¨é‡å¾ˆå¥½ï¼Œéå¸¸å–œæ¬¢ï¼"
    # run_agent(positive_review)
    
    st.set_page_config(page_title="æ™ºèƒ½å®¢æœç³»ç»Ÿ", page_icon="ğŸ¤–", layout="wide")
    st.title("ğŸ¤– æ™ºèƒ½å®¢æœç³»ç»Ÿ")

    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")

        # 1. LLM é…ç½®
        st.subheader("1. å¤§è¯­è¨€æ¨¡å‹ (LLM)")
        llm_provider = st.selectbox("é€‰æ‹©æ¨¡å‹æä¾›å•†", ["DeepSeek", "OpenAI"])
        llm_api_key = st.text_input(f"è¾“å…¥ {llm_provider} API Key", type="password")

        # 2. RAG (çŸ¥è¯†åº“) é…ç½®
        st.subheader("2. çŸ¥è¯†åº“ (PDF)")
        uploaded_file = st.file_uploader("ä¸Šä¼ äº§å“è¯´æ˜ä¹¦ (PDF)", type="pdf")
        
        st.subheader("3. åµŒå…¥æ¨¡å‹")
        embedding_provider = st.selectbox("é€‰æ‹©åµŒå…¥æ¨¡å‹æä¾›å•†", ["DashScope (Alibaba)", "OpenAI"])
        embedding_api_key = st.text_input(f"è¾“å…¥ {embedding_provider} API Key", type="password")

        # 4. åº”ç”¨é…ç½®æŒ‰é’®
        if st.button("åº”ç”¨é…ç½®", use_container_width=True):
            if not all([llm_api_key, uploaded_file, embedding_api_key]):
                st.error("è¯·å¡«å†™æ‰€æœ‰API Keyå¹¶ä¸Šä¼ PDFæ–‡ä»¶ï¼")
            else:
                with st.spinner("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿï¼Œè¯·ç¨å€™..."):
                    # åˆ›å»ºä¸´æ—¶ç›®å½•å¹¶ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    temp_dir = "temp"
                    os.makedirs(temp_dir, exist_ok=True)
                    pdf_path = os.path.join(temp_dir, uploaded_file.name)
                    with open(pdf_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # 1. åˆå§‹åŒ– LLM
                    if llm_provider == "DeepSeek":
                        llm = init_chat_model(
                            "deepseek-chat",
                            model_provider="deepseek",
                            base_url="https://api.deepseek.com",
                            api_key= llm_api_key,
                            model_kwargs={"response_format": {"type": "json_object"}}
                        )
                    else: # OpenAI
                        llm = ChatOpenAI(model="gpt-4o", api_key=llm_api_key, temperature=0,model_kwargs={"response_format": {"type": "json_object"}})
                    
                    # 2. åˆå§‹åŒ– RAG å·¥å…·
                    faiss_path = os.path.join(temp_dir, "faiss_index")
                    vector_store = get_vector_store(pdf_path, faiss_path, embedding_provider, embedding_api_key)
                    tools = create_rag_tool(vector_store)
                    
                    # 3. åˆ›å»ºå¹¶ç¼–è¯‘ Agent å›¾
                    app = create_graph(llm, tools)
                    
                    # 4. å°†ç¼–è¯‘å¥½çš„ app å­˜å…¥ session_state
                    st.session_state.app = app
                    st.session_state.configured = True
                    st.success("ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼å¯ä»¥å¼€å§‹èŠå¤©äº†ã€‚")
                    st.session_state.messages = [{"role": "assistant", "content": "ç³»ç»Ÿå·²å°±ç»ªï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"}]
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ å®Œæˆé…ç½®åï¼Œä¸æˆ‘å¼€å§‹å¯¹è¯ã€‚"}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input():
        if not st.session_state.get("configured", False):
            st.error("è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ å®Œæˆé…ç½®å¹¶ç‚¹å‡»'åº”ç”¨é…ç½®'æŒ‰é’®ï¼")
        else:
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)
            
            with st.chat_message("assistant"):
                with st.spinner("æ€è€ƒä¸­..."):
                    app = st.session_state.app
                    inputs = {"original_review": prompt}
                    result = app.invoke(inputs)
                    response = result.get("finally_reply", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚")
                    st.write(response)
            
            st.session_state.messages.append({"role": "assistant", "content": response})